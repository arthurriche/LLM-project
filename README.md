# LLM Project - Fine-tuning et Tokenization

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-yellow.svg)](https://huggingface.co/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## üéØ Overview

Ce projet d√©montre l'expertise en Large Language Models (LLMs) √† travers plusieurs notebooks Jupyter couvrant le fine-tuning, la tokenization, et l'utilisation de mod√®les de pointe comme Mistral 7B. Ce projet fait partie du cours MVA (Master en Vision Artificielle) et illustre les techniques avanc√©es de traitement du langage naturel.

## üöÄ Key Features

### Fine-tuning Mistral 7B
- **LoRA Fine-tuning** - Adaptation efficace de Mistral 7B avec LoRA (Low-Rank Adaptation)
- **Dataset Preparation** - Pr√©paration et formatage des donn√©es pour l'entra√Ænement
- **Model Download** - T√©l√©chargement et configuration du mod√®le Mistral 7B v0.3
- **Training Pipeline** - Pipeline complet d'entra√Ænement avec validation

### Tokenization et Transformers
- **Tokenization Analysis** - Analyse approfondie des techniques de tokenization
- **Transformer Architecture** - Impl√©mentation et compr√©hension des Transformers
- **Text Processing** - Traitement et pr√©paration des donn√©es textuelles

### Tests et √âvaluation
- **Model Testing** - Tests complets des mod√®les fine-tun√©s
- **Performance Evaluation** - √âvaluation des performances et m√©triques
- **Interactive Demos** - D√©monstrations interactives des capacit√©s

## üìÅ Project Structure

```
LLM-project/
‚îú‚îÄ‚îÄ mistral_finetune_7b.ipynb      # Fine-tuning complet de Mistral 7B
‚îú‚îÄ‚îÄ LLM_Tokenization_Finetuning.ipynb  # Tokenization et fine-tuning
‚îú‚îÄ‚îÄ Test_LLM.ipynb                 # Tests et √©valuation des mod√®les
‚îú‚îÄ‚îÄ Trasnformers.ipynb             # Architecture et impl√©mentation Transformers
‚îú‚îÄ‚îÄ session2.ipynb                 # Session pratique suppl√©mentaire
‚îî‚îÄ‚îÄ README.md                      # Documentation du projet
```

## üõ†Ô∏è Installation

### Pr√©requis
- Python 3.8+
- Jupyter Notebook ou JupyterLab
- Compte Google Colab Pro+ (recommand√© pour l'entra√Ænement)
- GPU A100 avec 40GB RAM (pour le fine-tuning)

### Installation des d√©pendances

```bash
# Cloner le repository
git clone https://github.com/arthurriche/LLM-project.git
cd LLM-project

# Installer les d√©pendances
pip install torch==2.2
pip install transformers
pip install datasets
pip install accelerate
pip install peft
pip install bitsandbytes
pip install huggingface_hub
```

## üìà Quick Start

### 1. Fine-tuning Mistral 7B

```python
# Ouvrir le notebook principal
jupyter notebook mistral_finetune_7b.ipynb

# Ou utiliser Google Colab
# Le notebook inclut un lien direct vers Colab
```

### 2. Tokenization et Transformers

```python
# Analyser la tokenization
jupyter notebook LLM_Tokenization_Finetuning.ipynb

# Explorer l'architecture Transformers
jupyter notebook Trasnformers.ipynb
```

### 3. Tests et √âvaluation

```python
# Tester les mod√®les
jupyter notebook Test_LLM.ipynb
```

## üßÆ Technical Implementation

### LoRA Fine-tuning
Le projet utilise LoRA (Low-Rank Adaptation) pour adapter efficacement Mistral 7B :

```python
# Configuration LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```

### Dataset Preparation
Formatage des donn√©es selon les sp√©cifications Mistral :

```python
# Format JSONL requis
{
    "messages": [
        {"role": "user", "content": "Question utilisateur"},
        {"role": "assistant", "content": "R√©ponse assistant"}
    ]
}
```

### Model Architecture
- **Mistral 7B v0.3** - Mod√®le de base avec 7 milliards de param√®tres
- **Sliding Window Attention** - M√©canisme d'attention optimis√©
- **Grouped-query Attention** - Attention par groupes pour l'efficacit√©

## üìä Performance Metrics

### Fine-tuning Results
- **Training Loss**: R√©duction progressive de la perte d'entra√Ænement
- **Validation Loss**: Performance sur l'ensemble de validation
- **Memory Usage**: Optimisation de l'utilisation m√©moire avec LoRA
- **Training Speed**: Acc√©l√©ration gr√¢ce aux techniques d'optimisation

### Model Evaluation
- **Perplexity**: Mesure de la qualit√© du langage g√©n√©r√©
- **BLEU Score**: √âvaluation de la traduction (si applicable)
- **Human Evaluation**: √âvaluation qualitative des r√©ponses

## üî¨ Advanced Features

### Memory Optimization
- **LoRA** - R√©duction drastique de l'utilisation m√©moire
- **Gradient Checkpointing** - Optimisation du gradient
- **Mixed Precision Training** - Entra√Ænement en pr√©cision mixte

### Data Processing
- **Dynamic Padding** - Padding dynamique pour l'efficacit√©
- **Data Streaming** - Traitement en streaming pour les gros datasets
- **Multi-GPU Training** - Entra√Ænement distribu√©

### Model Serving
- **Quantization** - R√©duction de la taille du mod√®le
- **Inference Optimization** - Optimisation de l'inf√©rence
- **API Integration** - Int√©gration API pour le d√©ploiement

## üöÄ Deployment

### Local Deployment
```bash
# Sauvegarder le mod√®le fine-tun√©
model.save_pretrained("./fine_tuned_mistral")

# Charger pour l'inf√©rence
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("./fine_tuned_mistral")
```

### Cloud Deployment
- **Google Colab** - Pour le d√©veloppement et les tests
- **AWS/GCP** - Pour le d√©ploiement en production
- **Hugging Face Spaces** - Pour le partage et la d√©monstration

## üìö Learning Resources

### Cours MVA
Ce projet fait partie du cours MVA sur les Large Language Models, couvrant :
- **Architecture des Transformers**
- **Techniques de Fine-tuning**
- **Optimisation des performances**
- **Applications pratiques**

### Documentation
- [Mistral AI Documentation](https://docs.mistral.ai/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [PEFT Documentation](https://huggingface.co/docs/peft/)

## ü§ù Contributing

1. Fork le repository
2. Cr√©er une branche feature (`git checkout -b feature/amazing-feature`)
3. Commit les changements (`git commit -m 'Add amazing feature'`)
4. Push vers la branche (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request

## üìù License

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üë®‚Äçüíª Author

**Arthur Riche**
- LinkedIn: [Arthur Riche](https://www.linkedin.com/in/arthurriche/)
- Email: arthur.riche@example.com

## üôè Acknowledgments

- **Mistral AI** pour le mod√®le Mistral 7B
- **Hugging Face** pour les outils et biblioth√®ques
- **√âcole MVA** pour le cadre acad√©mique
- **Google Colab** pour l'infrastructure de calcul

---

‚≠ê **Star ce repository si vous le trouvez utile !**

*Ce projet d√©montre l'expertise en Large Language Models et les techniques avanc√©es de fine-tuning pour des applications pratiques.*
